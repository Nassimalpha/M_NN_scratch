{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_obs = 500\n",
    "x_mat_1 = np.random.uniform(-1,1,size = (num_obs,2))\n",
    "x_mat_bias = np.ones((num_obs,1))\n",
    "x_mat_full = np.concatenate( (x_mat_1,x_mat_bias), axis=1)\n",
    "\n",
    "# # Diamond Pattern\n",
    "y = ((np.abs(x_mat_full[:,0]) + np.abs(x_mat_full[:,1]))<1).astype(int)\n",
    "\n",
    "# number of inputs\n",
    "n_x = 2  # +1 for bias\n",
    "# number of layers\n",
    "n_layers = 3\n",
    "# number of hdden layer nodes\n",
    "n_hiddenL_nodes = 4\n",
    "# number of outputs\n",
    "n_y = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Forward_prop(w1, w2):\n",
    "    \"\"\" forward propagation and gradient computation \"\"\"\n",
    "    \n",
    "    global x_mat_full\n",
    "    global y\n",
    "    \n",
    "#     Feed Forward : \n",
    "    z2 = np.dot(x_mat_full, w1)\n",
    "    a2 = sigmoid(z2)\n",
    "    z3 = np.dot(a2, w2)\n",
    "    y_pred = sigmoid(z3)\n",
    "    \n",
    "#     computing gradients :\n",
    "    J = y_pred - y\n",
    "    J_W2_grad = np.dot(J, a2)\n",
    "    \n",
    "    J_a2_grad = np.dot(J.reshape(-1,1), w2.reshape(-1,1).T)\n",
    "    a2_z2_grad = sigmoid(z2) * (1 - sigmoid(z2))\n",
    "    J_W1_grad = (J_a2_grad * a2_z2_grad).T.dot(x_mat_full).T\n",
    "    gradient = (J_W1_grad, J_W2_grad)\n",
    "                        \n",
    "    return y_pred, gradient\n",
    "    \n",
    "    \n",
    "# loss fu\n",
    "def RMSE(y_pred, y):\n",
    "    \"\"\" Root mean squared error \"\"\"\n",
    "    return np.sqrt(np.sum((y_pred - y)**2)) / len(y_pred)\n",
    "\n",
    "#  Or logaritmic loss\n",
    "\n",
    "def loss_fn(y_true, y_pred, eps=1e-16):\n",
    "    \"\"\"\n",
    "    Loss function we would like to optimize (minimize)\n",
    "    We are using Logarithmic Loss\n",
    "    http://scikit-learn.org/stable/modules/model_evaluation.html#log-loss\n",
    "    \"\"\"\n",
    "    y_pred = np.maximum(y_pred,eps)\n",
    "    y_pred = np.minimum(y_pred,(1-eps))\n",
    "    return -(np.sum(y_true * np.log(y_pred)) + np.sum((1-y_true)*np.log(1-y_pred)))/len(y_true)\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"\"  The Sigmoid function \"\"\" \n",
    "    return (1 / (1 + np.exp(-z)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_func_value | Accuracy\n",
      "0.02263883497620577 | 0.49\n",
      "0.023022881300889357 | 0.51\n",
      "0.023737241095103508 | 0.49\n",
      "0.024243556986374853 | 0.51\n",
      "0.0237045078774777 | 0.49\n",
      "0.022680399575733458 | 0.51\n",
      "0.022368196962606023 | 0.49\n",
      "0.02234670913719681 | 0.512\n",
      "0.022343460258788212 | 0.646\n",
      "0.022340927341649663 | 0.646\n",
      "0.022338416186393494 | 0.658\n",
      "0.022335864914968667 | 0.656\n",
      "0.022333243405644913 | 0.644\n",
      "0.022330521708323944 | 0.632\n",
      "0.02232767035879372 | 0.628\n",
      "0.022324659147418918 | 0.626\n",
      "0.022321456556982318 | 0.628\n",
      "0.022318029078782914 | 0.614\n",
      "0.02231434055638247 | 0.612\n",
      "0.022310351497046604 | 0.608\n",
      "0.022306018343588162 | 0.602\n",
      "0.022301292690046638 | 0.594\n",
      "0.022296120429209764 | 0.594\n",
      "0.022290440821384334 | 0.592\n",
      "0.02228418547626616 | 0.592\n",
      "0.022277277242577827 | 0.592\n",
      "0.022269629003729537 | 0.586\n",
      "0.022261142382181236 | 0.584\n",
      "0.022251706360441815 | 0.588\n",
      "0.02224119583252007 | 0.584\n",
      "0.022229470105641868 | 0.584\n",
      "0.022216371377285007 | 0.584\n",
      "0.022201723215721763 | 0.584\n",
      "0.022185329071517716 | 0.588\n",
      "0.02216697084073996 | 0.59\n",
      "0.0221464074860031 | 0.596\n",
      "0.02212337369778186 | 0.598\n",
      "0.022097578546463686 | 0.598\n",
      "0.022068704039717224 | 0.6\n",
      "0.022036403469490238 | 0.598\n",
      "0.022000299424785194 | 0.598\n",
      "0.02195998138443725 | 0.602\n",
      "0.021915002919233203 | 0.61\n",
      "0.021864878758098647 | 0.612\n",
      "0.02180908233554192 | 0.618\n",
      "0.021747044942602434 | 0.626\n",
      "0.021678158214565965 | 0.628\n",
      "0.0216017823025453 | 0.634\n",
      "0.021517262503024707 | 0.634\n",
      "0.021423957082941397 | 0.648\n",
      "0.02132127821607641 | 0.652\n",
      "0.02120874608098045 | 0.668\n",
      "0.021086053245385247 | 0.682\n",
      "0.020953132904981915 | 0.702\n",
      "0.020810221324109396 | 0.718\n",
      "0.020657903271831556 | 0.728\n",
      "0.020497130509046443 | 0.734\n",
      "0.020329207700131198 | 0.732\n",
      "0.020155746352787808 | 0.736\n",
      "0.019978593255697882 | 0.744\n",
      "0.01979974301894231 | 0.754\n",
      "0.019621243606697734 | 0.75\n",
      "0.01944510012177183 | 0.752\n",
      "0.01927317820817835 | 0.762\n",
      "0.019107107147762863 | 0.76\n",
      "0.018948185224141764 | 0.762\n",
      "0.01879729468445648 | 0.762\n",
      "0.01865483722922045 | 0.762\n",
      "0.018520700445794163 | 0.764\n",
      "0.01839426060217778 | 0.764\n",
      "0.018274420137267204 | 0.766\n",
      "0.018159672438459742 | 0.77\n",
      "0.01804818441985408 | 0.77\n",
      "0.017937889302998233 | 0.772\n",
      "0.017826586507286274 | 0.778\n",
      "0.01771205060243388 | 0.78\n",
      "0.017592154735716715 | 0.778\n",
      "0.017465013866980156 | 0.782\n",
      "0.017329147868464352 | 0.786\n",
      "0.017183653345697016 | 0.79\n",
      "0.017028357716372802 | 0.796\n",
      "0.01686391566360019 | 0.794\n",
      "0.016691806375064743 | 0.794\n",
      "0.016514207658525258 | 0.8\n",
      "0.016333758107493063 | 0.802\n",
      "0.01615325469050322 | 0.806\n",
      "0.01597535309823412 | 0.818\n",
      "0.015802328828717217 | 0.816\n",
      "0.015635934687476635 | 0.824\n",
      "0.015477350740468706 | 0.822\n",
      "0.015327221716712651 | 0.824\n",
      "0.015185722456263926 | 0.828\n",
      "0.015052693866747333 | 0.836\n",
      "0.014927672197419771 | 0.846\n",
      "0.014810153444696694 | 0.852\n",
      "0.014699274817757962 | 0.856\n",
      "0.014594790987774435 | 0.86\n",
      "0.014494967496922128 | 0.868\n",
      "0.014401796492833097 | 0.868\n",
      "0.014309125377532211 | 0.874\n"
     ]
    }
   ],
   "source": [
    "# initializing params\n",
    "\n",
    "w1 = np.random.uniform(-1,1,size = (3,4))\n",
    "w2 = np.random.uniform(-1,1,size = (4,))\n",
    "num_iter = 100\n",
    "learning_rate = 0.02\n",
    "\n",
    "loss_vals, accuracies = [], []\n",
    "for i in range(num_iter):\n",
    "    # forward computation, and get the gradient\n",
    "    y_pred, gradient = Forward_prop(w1,w2)\n",
    "    \n",
    "    # Update the weight matrices\n",
    "    w1 -= learning_rate * gradient[0]\n",
    "    w2 -= learning_rate * gradient[1]    \n",
    "    \n",
    "    # Compute the loss and accuracy\n",
    "#     loss_val = loss_fn(y, y_pred)\n",
    "    loss_val = RMSE(y_pred, y)\n",
    "    loss_vals.append(loss_val)\n",
    "    \n",
    "    from sklearn.metrics import accuracy_score\n",
    "    accuracy = accuracy_score(y, y_pred.round())\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "    \n",
    "    if i == 0:\n",
    "        print('Loss_func_value', '| Accuracy')\n",
    "        \n",
    "    print('{} | {}'.format(loss_val, accuracy))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
